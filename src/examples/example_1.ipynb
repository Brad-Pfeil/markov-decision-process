{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Simple Markov Chain\n",
    "\n",
    "Here we'll use a simple two state markov chain with an absorbing state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Markov Decision Problem](../../assets/example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state and action spaces are:\n",
    "\n",
    "$$ S = \\{ \\mathrm{S0}, \\mathrm{S1} \\}$$\n",
    "$$ A = \\{ \\mathrm{A0}, \\mathrm{A1} \\}$$\n",
    "\n",
    "Note that if we're in state S0, we can choose either A0 or A1. A0 has a half chance of leaving us in S0 and giving payoff 5, and a half chance of moving us to state S1 with payoff 5. A1 moves us directly to S1 and we get payoff 10.\n",
    "\n",
    "When I'm in state S1, I can only choose action A1, which leaves me in S1 and gives -1 payoff.\n",
    "\n",
    "\n",
    "Our transition and reward matrices are:\n",
    "\n",
    "Action 0\n",
    "\n",
    "$$ P_{0} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix} \\quad R_{0} = \\begin{pmatrix} 5 & 5 \\\\ 0 & 0 \\end{pmatrix}$$\n",
    "\n",
    "Action 1\n",
    "\n",
    "$$ P_{0} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 1 \\end{pmatrix} \\quad R_{0} = \\begin{pmatrix} 0 & 10 \\\\ 0 & -1 \\end{pmatrix}$$\n",
    "\n",
    "\n",
    "**Be aware of:**\n",
    "\n",
    "The PyMDPToolbox package we use to solve the problems require that the transition matrices be valid Markov matrices (rows must sum to 1).\n",
    "\n",
    "In this setup we are unable to choose action A0 if we're in state S1. So the transition matrix for this action has a row of 0s in the \"current state = S1\" row.\n",
    "\n",
    "To fix this, we find rows that sum to 0; place a 1 on the diagonal elements; and replace the corresponding reward with a very negative number.\n",
    "\n",
    "We should choose that negative number to be something smaller than the smallest payoff; like -1e-8. We can't set it to - $\\infty$, because then after matrix multiplications everything will come out as infinities.\n",
    "\n",
    "Our new transition and reward matrices are:\n",
    "\n",
    "\n",
    "Action 0\n",
    "\n",
    "$$ P_{0} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 1 \\end{pmatrix} \\quad R_{0} = \\begin{pmatrix} 5 & 5 \\\\ 0 & -1e8 \\end{pmatrix}$$\n",
    "\n",
    "Action 1\n",
    "\n",
    "$$ P_{0} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 1 \\end{pmatrix} \\quad R_{0} = \\begin{pmatrix} 0 & 10 \\\\ 0 & -1 \\end{pmatrix}$$\n",
    "\n",
    "\n",
    "**Solution**\n",
    "\n",
    "If this problem goes for one period, then the solution should be that we pick action A1 no matter what state we're in. If we're in state 1, A0 gives expected reward of 5; whereas A1 gives expected reward of 10. In state S1, we can only pick A1 which gives reward -1. So our policy and value functions for a one period problem are:\n",
    "\n",
    "$$\\mathrm{policy} = \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} \\quad \\mathrm{value}_{0} = \\begin{bmatrix}10 \\\\ -1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "If this is a two period problem, $t \\in \\{1, 0\\}$, then the solution is\n",
    "\n",
    "$$\\mathrm{policy} = \\begin{bmatrix} 0 & 1\\\\ 1 & 1 \\end{bmatrix} \\quad \\mathrm{value}_{0} = \\begin{bmatrix} 9.5 & 10 \\\\ -2 & -1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Let's verify that we get these solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from markov_decision_process import TimeAugmentedMDP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"deep\")\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Set up problem\n",
    "\n",
    "\n",
    "The class must be initialized with lists for S (state space), A (action space), and T (time intervals).\n",
    "\n",
    "Define functions called `transition` and `reward`. These take the variables s_prime, s, t, a. (next state, current state, current time, action).\n",
    "\n",
    "transition returns the transition probability for that event. reward returns the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states: list[str] = [\"S0\", \"S1\"]\n",
    "actions: list[str] = [\"A0\", \"A1\"]\n",
    "times: list[int] = [-1, 0]\n",
    "\n",
    "\n",
    "def transition(s_prime, s, a, t):\n",
    "    if a == \"A0\":\n",
    "        if s == \"S0\" and s_prime == \"S0\":\n",
    "            return 0.5\n",
    "        if s == \"S0\" and s_prime == \"S1\":\n",
    "            return 0.5\n",
    "\n",
    "    if a == \"A1\":\n",
    "        if s == \"S0\" and s_prime == \"S1\":\n",
    "            return 1\n",
    "        if s == \"S1\" and s_prime == \"S1\":\n",
    "            return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def reward(s_prime, s, a, t):\n",
    "    if a == \"A0\":\n",
    "        if s == \"S0\" and s_prime == \"S0\":\n",
    "            return 5\n",
    "        if s == \"S0\" and s_prime == \"S1\":\n",
    "            return 5\n",
    "\n",
    "    if a == \"A1\":\n",
    "        if s == \"S0\" and s_prime == \"S1\":\n",
    "            return 10\n",
    "        if s == \"S1\" and s_prime == \"S1\":\n",
    "            return -1\n",
    "\n",
    "    return -1e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Run the necessary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class we're leveraging is TimeAugmentedMDP\n",
    "\n",
    "It requires some inputs. Naturally:\n",
    "\n",
    "- states: list[int] | list[float] | list[str]\n",
    "- actions: list[int] | list[float] | list[str]\n",
    "- times: list[int]\n",
    "\n",
    "States and actions can be strings or numerics, but time must be increasing consecutive integers.\n",
    "\n",
    "\n",
    "We need to specify a reward function that is a function of s_prime, s, a, t; exactly in that order with those variable names. \n",
    "\n",
    "We can provide a transition function, with the same arguments as the reward functions; but where the outputs are the probabilities of transitioning to state s_prime given we're in state s, time t, and take action a.\n",
    "\n",
    "If we don't provide a transition function, we have to provide a model object with a predict_proba() method. We'll see an example of that later.\n",
    "\n",
    "\n",
    "An interesting option is the `state_space_data_path` argument. This is for that massive cartesian product of states x states x actions x times. Since that can get large, and take up quite a bit of compute time; we compute it with Polars and write it as partitioned parquet to disk. If no argument is supplied, we use '/tmp/state_space_data/'. \n",
    "\n",
    "Passing the `force_overwrite` option deletes the previous data and reconstructs it. You'll need this if you tweak your state space/times/actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:markov_decision_process.time_augmented_mdp:Mode not set. Inferring mode...\n",
      "INFO:markov_decision_process.time_augmented_mdp:Reward function is vectorized: False\n",
      "INFO:markov_decision_process.time_augmented_mdp:Transition function is vectorized: False\n",
      "INFO:markov_decision_process.time_augmented_mdp:Reward and transition functions are not vectorized. Setting mode to \"flexible\"\n",
      "INFO:markov_decision_process.time_augmented_mdp:State space augmented with time\n",
      "INFO:markov_decision_process.time_augmented_mdp:Generating state space data...\n",
      "WARNING:markov_decision_process.time_augmented_mdp:Force overwrite active. Deleting existing directory and recreating...\n",
      "INFO:markov_decision_process.time_augmented_mdp:No data found on disk. Generating...\n",
      "INFO:markov_decision_process.time_augmented_mdp:State space data generated and saved to disk\n",
      "INFO:markov_decision_process.time_augmented_mdp:Generating rewards and transitions...\n"
     ]
    },
    {
     "ename": "ComputeError",
     "evalue": "OverflowError: can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mComputeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m mdp = TimeAugmentedMDP(\n\u001b[32m      2\u001b[39m     states=states,\n\u001b[32m      3\u001b[39m     actions=actions,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     force_overwrite=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmdp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPolicy:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m pprint(mdp.policy_function, expand_all=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Markov-Decision-Process-Toolkit/src/markov_decision_process/time_augmented_mdp.py:591\u001b[39m, in \u001b[36mTimeAugmentedMDP.solve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msolve\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_rewards_and_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m._enforce_valid_matrices()\n\u001b[32m    594\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.times)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Markov-Decision-Process-Toolkit/src/markov_decision_process/time_augmented_mdp.py:519\u001b[39m, in \u001b[36mTimeAugmentedMDP.build_rewards_and_transitions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    516\u001b[39m df_a = df_a.filter(pl.col(\u001b[33m\"\u001b[39m\u001b[33mt_prime\u001b[39m\u001b[33m\"\u001b[39m) <= \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.times))\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Contruct the transition and reward matrices\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m P, R = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__build_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m transitions.append(P)\n\u001b[32m    522\u001b[39m rewards.append(R)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Markov-Decision-Process-Toolkit/src/markov_decision_process/time_augmented_mdp.py:368\u001b[39m, in \u001b[36mTimeAugmentedMDP.__build_matrix\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03mGiven a data frame with columns s_prime, s, t, t_prime, probability,\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03mand reward, build a transition or reward matrix.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    360\u001b[39m \u001b[33;03m    A sparse matrix representing the transition or reward matrix.\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# Each row has s_prime, s, t, t_prime. Add two new columnns,\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# s_augmented_index and s_prime_augmented_index. These are the indices\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# of the augmented state space for tuples (s,t) and (s_prime, t_prime).\u001b[39;00m\n\u001b[32m    366\u001b[39m \n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# Create new columns by applying the mapping\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_elements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maugmented_state_to_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m                \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUInt64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms_augmented_index\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m df = df.with_columns(\n\u001b[32m    381\u001b[39m     [\n\u001b[32m    382\u001b[39m         pl.struct([\u001b[33m\"\u001b[39m\u001b[33ms_prime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mt_prime\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     ]\n\u001b[32m    391\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Create a sparse matrix with dimension len(S_augmented) x\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;66;03m# len(S_augmented). Note that we need the indices in S_augmented\u001b[39;00m\n\u001b[32m    395\u001b[39m \u001b[38;5;66;03m# corresponding to (s, t) and (s_prime, t_prime).\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# Create a column called 'index' for what the index of (s,t) is.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Markov-Decision-Process-Toolkit/.venv/lib/python3.12/site-packages/polars/dataframe/frame.py:9805\u001b[39m, in \u001b[36mDataFrame.with_columns\u001b[39m\u001b[34m(self, *exprs, **named_exprs)\u001b[39m\n\u001b[32m   9659\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_columns\u001b[39m(\n\u001b[32m   9660\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   9661\u001b[39m     *exprs: IntoExpr | Iterable[IntoExpr],\n\u001b[32m   9662\u001b[39m     **named_exprs: IntoExpr,\n\u001b[32m   9663\u001b[39m ) -> DataFrame:\n\u001b[32m   9664\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9665\u001b[39m \u001b[33;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[32m   9666\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   9803\u001b[39m \u001b[33;03m    └─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[32m   9804\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m9805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Markov-Decision-Process-Toolkit/.venv/lib/python3.12/site-packages/polars/lazyframe/frame.py:2065\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, streaming, engine, background, _check_order, _eager, **_kwargs)\u001b[39m\n\u001b[32m   2063\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2064\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2065\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mComputeError\u001b[39m: OverflowError: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "mdp = TimeAugmentedMDP(\n",
    "    states=states,\n",
    "    actions=actions,\n",
    "    times=times,\n",
    "    reward_function=reward,\n",
    "    transition_function=transition,\n",
    "    state_space_data_path=\"/tmp/state_space_data/\",\n",
    "    force_overwrite=True,\n",
    ")\n",
    "\n",
    "mdp.solve()\n",
    "\n",
    "print(\"Policy:\")\n",
    "pprint(mdp.policy_function, expand_all=True)\n",
    "\n",
    "print(\"Value Function:\")\n",
    "pprint(mdp.value_function, expand_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot the transition matrix for actions A1 and A2\n",
    "mdp.plot_matrix(\n",
    "    matrix_type=\"transitions\", t=0, a=\"A0\", ax=ax[0, 0], annotate=True\n",
    ")\n",
    "mdp.plot_matrix(\n",
    "    matrix_type=\"transitions\", t=0, a=\"A1\", ax=ax[0, 1], annotate=True\n",
    ")\n",
    "mdp.plot_matrix(matrix_type=\"rewards\", t=0, a=\"A0\", ax=ax[1, 0], annotate=True)\n",
    "mdp.plot_matrix(matrix_type=\"rewards\", t=0, a=\"A1\", ax=ax[1, 1], annotate=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
